<!DOCTYPE html>
<html>
	<head>
        <link href="https://fonts.googleapis.com/css?family=M+PLUS+Rounded+1c" rel="stylesheet">
		<title>6.S198 Assignment 5</title>
    </head>

	<body style="font-family: 'M PLUS Rounded 1c', sans-serif;">
		<h1>
			6.S198 Assignment 5
		</h1>
		<p>
			Jenny Xue jennyxue@mit.edu
		</p>
		<div>
			<h3> 
                1.2) Changing the Model
            </h3>
        </div>

        <div>
            <ol>
                <li><b>
                    1.2.1 Why does the Generator need the second FC layer to transform the shape [256] output of the first FC layer? 
                    Hint:  Can a Reshape layer reshape [600] to [20,20,2]? How about [800] to [20,10,4]?
                </b></li>
                <ul>
                    <li>
                        A reshape layer cannot reshape [600] to [20, 20, 2]. However, it can reshape from [800] to [20, 10, 4].
                    </li>
                </ul>
                <li><b>
                    1.2.2  Submit screenshots of some results (generated examples, discriminator predictions on real and fake data, and learning curves as in the plots above).
                </b></li>
                <ul>
                    <li>
                        Fully connected model: 211380 Training Examples.
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/1.png" width="700" height="auto">
                </div>
                <ul>
                    <li>
                        Convolutional Model: 50505 Training Examples.
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/2.png" width="700" height="auto">
                </div>
                <li><b>
                        1.2.3  Can you say anything about the performance of the system's default fully connected model versus the performance of the convolutional model? 
                </b></li>
                <ul>
                    <li>
                        The convolutional model was able to achieve what the fully connected model did but with significantly less training examples.
                    </li>
                </ul>
            </ol>
        </div>

        <div>
			<h3> 
                1.3) Exploring with the GAN Playground 
            </h3>
        </div>

        <div>
            <ol>
                <li><b>
                        Submit screenshots of your results with at least 3 different configurations (architecture, learning rate, optimizers). 
                </b></li>
                <ul>
                    <li>
                        Convolutional model, learning rate 0.1, 113955 training examples to achieve reasonable MNIST digits, performed better than fully connected model. 
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/3.png" width="700" height="auto">
                </div>
                <ul>
                    <li>
                        Convolutional model, convolution output 32, learning rate 0.1, optimizer sgd, batch size 20, 502220 training examples to achieve reasonable MNIST digits.
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/4.png" width="700" height="auto">
                </div>
                <ul>
                    <li>
                        Convolutional model, convolution output 32, learning rate 0.1, optimizer adagrad, batch size 20, it was not able to achieve reasonable MNIST digits after 63560 training examples.
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/5.png" width="700" height="auto">
                </div>
                <li><b>
                    1.3.1. Were any of your models able to generate any reasonable MNIST digits? If so, were any of your models able to generate all of the MNIST digits (0-9)? Did any of your models get stuck at some point generating one or a few digits only (i.e. mode-collapse)?
                </b></li>
                <ul>
                    <li>
                        Most models were all able to generate some MNIST digits, the most frequent digit being 1. My last model struggled to generate reasonable images after 60000 examples. None of my models got stuck. 
                    </li>
                </ul>
                <li>
                    <b>
                        1.3.2 What happened when the discriminator learning rate was greater than the generator learning rate? What about when the generator learning rate was greater than the discriminator learning rate?
                    </b>
                </li>
                <ul>
                    <li>
                        Discriminator > Generator: prioritize discriminator
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/6.png" width="700" height="auto">
                </div>
                <ul>
                    <li>
                        Generator > Discriminator: prioritize generator 
                    </li>
                </ul>
                <div style="text-align:center">
                    <img src="assignment-5/7.jpeg" width="700" height="auto">
                </div>
                <li><b>
                        1.3.3 Try building configurations for CIFAR. (This will require a miinute or two to loadFirst, try running with only FC layers for ~15 minutes and document your results.
                </b></li>
                <div style="text-align:center">
                    <img src="assignment-5/cifar.png" width="700" height="auto">
                </div>
                <ul>
                    <li>
                        Cifar is more complex because of the colors which is why the images I generated are not good.
                    </li>
                </ul>
            </ol>
        </div>