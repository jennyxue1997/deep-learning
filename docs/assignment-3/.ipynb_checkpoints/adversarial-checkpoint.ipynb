{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesizing [adversarial examples](https://arxiv.org/abs/1312.6199) for neural networks is surprisingly easy: small, carefully-crafted perturbations to inputs can cause neural networks to misclassify inputs in arbitrarily chosen ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup\n",
    "\n",
    "We choose to attack an [Inception v3](https://arxiv.org/abs/1512.00567) network trained on [ImageNet](http://www.image-net.org/). In this section, we load a pre-trained network from the [TF-slim image classification library](https://github.com/tensorflow/models/tree/master/slim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the input image to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (299, 299, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Inception v3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception(image, reuse):\n",
    "    preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)\n",
    "    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        logits, _ = nets.inception.inception_v3(\n",
    "            preprocessed, 1001, is_training=False, reuse=reuse)\n",
    "        logits = logits[:,1:] # ignore background class\n",
    "        probs = tf.nn.softmax(logits) # probabilities\n",
    "    return logits, probs\n",
    "\n",
    "logits, probs = inception(x, reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, `logits` is the output of the classifier. Logits are pre-softmax values, so to get an actual probability distribution, we'd have to compute `softmax(logits)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pre-trained weights. This Inception v3 has a top-5 accuracy of 93.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = tempfile.mkdtemp()\n",
    "inception_tarball, _ = urlretrieve(\n",
    "    'http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz')\n",
    "tarfile.open(inception_tarball, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restore_vars = [\n",
    "    var for var in tf.global_variables()\n",
    "    if var.name.startswith('InceptionV3/')\n",
    "]\n",
    "saver = tf.train.Saver(restore_vars)\n",
    "saver.restore(sess, os.path.join(data_dir, 'inception_v3.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write some code to show an image, classify it, and show the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imagenet_json, _ = urlretrieve(\n",
    "    'http://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
    "with open(imagenet_json) as f:\n",
    "    imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(img, correct_class=None, target_class=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "    fig.sca(ax1)\n",
    "    p = sess.run(probs, feed_dict={x: img})[0]\n",
    "    ax1.imshow(img)\n",
    "    fig.sca(ax1)\n",
    "    \n",
    "    topk = list(p.argsort()[-10:][::-1])\n",
    "    topprobs = p[topk]\n",
    "    barlist = ax2.bar(range(10), topprobs)\n",
    "    if target_class in topk:\n",
    "        barlist[topk.index(target_class)].set_color('r')\n",
    "    if correct_class in topk:\n",
    "        barlist[topk.index(correct_class)].set_color('g')\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10),\n",
    "               [imagenet_labels[i][:15] for i in topk],\n",
    "               rotation='vertical')\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image\n",
    "\n",
    "We load our example image and make sure it's classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use a different image below! If you need help figuring out the ImageNet class ID of your image, you can consult [this list](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path, _ = urlretrieve('http://www.anishathalye.com/media/2017/07/25/cat.jpg')\n",
    "img_class = 281 # true class\n",
    "img = PIL.Image.open(img_path)\n",
    "big_dim = max(img.width, img.height)\n",
    "wide = img.width > img.height\n",
    "new_w = 299 if not wide else int(img.width * 299 / img.height)\n",
    "new_h = 299 if wide else int(img.height * 299 / img.width)\n",
    "img = img.resize((new_w, new_h)).crop((0, 0, 299, 299))\n",
    "img = (np.asarray(img) / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify(img, correct_class=img_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial examples\n",
    "\n",
    "Given an image $\\mathbf{x}$, our neural network outputs a probability distribution over labels, $P(y \\mid \\mathbf{x})$. When we craft an adversarial input, we want to find an $\\hat{\\mathbf{x}}$ where $\\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$ is maximized for a target label $\\hat{y}$: that way, our input will be misclassified as the target class. We can ensure that $\\hat{\\mathbf{x}}$ doesn't look too different from the original $\\mathbf{x}$ by constraining ourselves to some $\\ell_\\infty$ box with radius $\\epsilon$, requiring that $\\left\\lVert \\mathbf{x} - \\hat{\\mathbf{x}} \\right\\rVert_\\infty \\le \\epsilon$.\n",
    "\n",
    "In this framework, an adversarial example is the solution to a constrained optimization problem that we can solve using [backpropagation](http://colah.github.io/posts/2015-08-Backprop/) and projected gradient descent, basically the same techniques that are used to train networks themselves. The algorithm is simple:\n",
    "\n",
    "We begin by initializing our adversarial example as $\\hat{\\mathbf{x}} \\leftarrow \\mathbf{x}$. Then, we repeat the following until convergence:\n",
    "\n",
    "1. $\\hat{\\mathbf{x}} \\leftarrow \\hat{\\mathbf{x}} + \\alpha \\cdot \\nabla \\log P(\\hat{y} \\mid \\hat{\\mathbf{x}})$\n",
    "2. $\\hat{\\mathbf{x}} \\leftarrow \\mathrm{clip}(\\hat{\\mathbf{x}}, \\mathbf{x} - \\epsilon, \\mathbf{x} + \\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining $\\log P(y \\mid x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code above gives us `logits`, which are the pre-softmax outputs of the classifier. To turn that into probabilities, and in particular, to compute $- \\log P(y \\mid x)$, we can use `tf.nn.sparse_softmax_cross_entropy_with_logits` (the reason for using this as opposed to using `softmax()` and then `log()` has to do with numerical stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, ()) # the adversarial target class\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=[y])\n",
    "# loss is now - log P (y | x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad, = tf.gradients(loss, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad` is now a tensor representing $\\nabla - \\log P(y \\mid x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running projected gradient descent (PGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to run the algorithm described above, where we alternate between gradient descent steps and projection steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints on how to use NumPy / TensorFlow to compute things that will be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can evaluate the gradient at a point by running:\n",
    "g = grad.eval({x: img, y: 123}) # what does the `y : 123` mean?\n",
    "\n",
    "# you can evaluate - log P(y | x) at a point by running:\n",
    "l = loss.eval({x: img, y: 123})\n",
    "# this can be useful for debugging\n",
    "\n",
    "# you can evaluate multiple tensors at once by running:\n",
    "g, l = sess.run([grad, loss], {x: img, y: 123})\n",
    "\n",
    "# you can clip a value by running:\n",
    "ex_clipped = np.clip([-0.1, 0.2, 0.7, 1.3], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attack parameters, feel free to experiment with these\n",
    "EPSILON = 0.05\n",
    "TARGET_CLASS = 924\n",
    "PGD_STEPS = 20\n",
    "STEP_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO implement the attack here to compute the adversarial example, ending up with the final result in `adv`\n",
    "\n",
    "adv = np.copy(img) # initialize adversarial example with original image\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the adversarial example\n",
    "\n",
    "Let's see if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify(adv, correct_class=img_class, target_class=TARGET_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things to try\n",
    "\n",
    "- Can you make an untargeted adversarial example? (find an image that's misclassified, but you don't care which class it's misclassified as, as long as it's not the true class).\n",
    "- Can you find a way to defend against adversarial examples? Perhaps try to find a function `purify(x)` that you use to preprocess images before you pass them to `classify(x)`? Some ideas to try are reducing the bit-depth of the image or JPEG-compressing the image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
